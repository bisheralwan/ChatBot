{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bisheralwan/ChatBot/blob/main/W2025/Assignments/A3/SYSC4415_W25_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcaEf9LdAu9k"
      },
      "source": [
        "# Welcome to Assignment 3\n",
        "\n",
        "**TA: [Igor Bogdanov](mailto:igorbogdanov@cmail.carleton.ca)**\n",
        "\n",
        "## General Instructions:\n",
        "\n",
        "This Assignment can be done **in a group of two or individually**.\n",
        "\n",
        "YOU HAVE TO JOIN A GROUP ON BRIGHTSPACE TO SUBMIT.\n",
        "\n",
        "Please state it explicitly at the beginning of the assignment.\n",
        "\n",
        "You need only one submission if it's group work.\n",
        "\n",
        "Please print out values when asked using Python's print() function with f-strings where possible.\n",
        "\n",
        "Submit your **saved notebook with all the outputs** to Brightspace, but ensure it will produce correct outputs upon restarting and click \"runtime\" → \"run all\" with clean outputs. Ensure your notebook displays all answers correctly.\n",
        "\n",
        "## Your Submission MUST contain your signature at the bottom.\n",
        "\n",
        "### Objective:\n",
        "In this assignment, we build a reasoning AI agent that facilitates ML operations and model evaluation. This assignment is heavily based on Tutorial 9.\n",
        "\n",
        "**Submission:** Submit your Notebook as a *.ipynb* file that adopts this naming convention: ***SYSC4415_W25_A3_NameLastname.ipynb*** on *Brightspace*. No other submission (e.g., through email) will be accepted. (Example file name: SYSC4415_W25_A3_IgorBogdanov.ipynb or SYSC4415_W25_A3_Student1_Student2.ipynb) The notebool MUST contain saved outputs\n",
        "\n",
        "**Runtime tips:**\n",
        "Agentic programming and API calling can be easily done locally and moved to Colab in the final stages, depending on the implementation of your tools and ML tasks you want to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIzWURYdCos_"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyRG5AEHNILq"
      },
      "source": [
        "Some basic libraries you need are imported here. Make sure you include whatever library you need in this entire notebook in the code block below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXYKklNMNpbQ"
      },
      "source": [
        "If you are using any library that requires installation, please paste the installation command here.\n",
        "Leave the code block below if you are not installing any libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NZEeRARqqPp8"
      },
      "outputs": [],
      "source": [
        "# Name: Bisher Abou-Alwan\n",
        "# Student Number: 101211242\n",
        "\n",
        "# Name: Mohammad Abusalem\n",
        "# Student Number: 101204665"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CrxZ_P9tNkln"
      },
      "outputs": [],
      "source": [
        "# Libraries to install - leave this code block blank if this does not apply to you\n",
        "# Please add a brief comment on why you need the library and what it does\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3_N_y3sNfTC",
        "outputId": "1c7a8652-27a3-408c-c6a5-9bcb8499944d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.22.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
            "Downloading groq-0.22.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n",
        "\n",
        "# Libraries you might need\n",
        "# General\n",
        "import os\n",
        "import zipfile\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# For pre-processing\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# For modeling\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import torchsummary\n",
        "\n",
        "# For metrics\n",
        "from sklearn.metrics import  accuracy_score\n",
        "from sklearn.metrics import  precision_score\n",
        "from sklearn.metrics import  recall_score\n",
        "from sklearn.metrics import  f1_score\n",
        "from sklearn.metrics import  classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import  roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Agent\n",
        "from groq import Groq\n",
        "from dataclasses import dataclass\n",
        "import re\n",
        "from typing import Dict, List, Optional\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vX4Z_nNI6BY"
      },
      "source": [
        "# Task 1: Registration and API Activation (5 marks)\n",
        "\n",
        "For this particular assignment, we will be using GroqCloud for LLM inference. This task aims to determine how to use the Groq API with LLMs.  \n",
        "\n",
        "Create a free account on https://groq.com/ and generate an API Key. Don't remove your key until you get your grade. Feel free to delete your API key after the term is completed.\n",
        "\n",
        "In conversational AI, prompting involves three key roles: the system role (which sets the agent's behavior and capabilities), the user role (which represents human inputs and queries), and the assistant role (which contains the agent's responses). The system role provides the foundational instructions and constraints, the user role delivers the actual queries or commands, and the assistant role generates contextual, step-by-step responses following the system's guidelines. This structured approach ensures consistent, controlled interactions where the agent maintains its defined behavior while responding to user needs, with each role serving a specific purpose in the conversation flow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5tSc4kOsghn_"
      },
      "outputs": [],
      "source": [
        "# Q1a (2 mark)\n",
        "# Create a client using your API key.\n",
        "\n",
        "client = Groq(api_key=\"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\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KT-vxP0QI74n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b939db5c-58e4-4c6b-f8b3-586f32249208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Penguins are indeed birds, but they have evolved to become flightless over time. There are several reasons for this:\n",
            "\n",
            "1. **Environmental pressures**: Penguins live in the Southern Hemisphere, where the climate is cold and the oceans are rich in food. As a result, they didn't need to fly to find food or escape predators. In fact, flying would have been energetically expensive and potentially hazardous in the strong winds and icy conditions.\n",
            "2. **Body shape and size**: Penguins have a unique body shape, with a streamlined torso, flippers, and a heavy skeleton. This shape is ideal for swimming and diving, but not for flying. Their wings are modified to be more suited for propulsion through water than through air.\n",
            "3. **Wing structure**: Penguins' wings are shorter and more rigid than those of flying birds. They have a solid, flipper-like shape that allows them to use their wings to \"fly\" through the water, but they lack the flexibility and lift needed to generate lift in the air.\n",
            "4. **Energy efficiency**: Flying is an energetically expensive activity, requiring a lot of energy to generate lift and stay aloft. Penguins have evolved to conserve energy by using their powerful flippers to swim and dive, which is more efficient for them.\n",
            "5. **Evolutionary trade-offs**: As penguins adapted to their aquatic environment, they likely traded off the ability to fly for other advantages, such as enhanced swimming and diving abilities. This is an example of an evolutionary trade-off, where the benefits of one trait (e.g., swimming) outweigh the costs of losing another trait (e.g., flying).\n",
            "\n",
            "Over time, these factors have led to the evolution of penguins as flightless birds, with their unique body shape, wing structure, and behaviors adapted to their aquatic environment.\n"
          ]
        }
      ],
      "source": [
        "# Q1b (3 marks)\n",
        "\n",
        "# instantiate chat_completion object using model of your choice (llama-3.3-70b-versatile - recommended)\n",
        "# Hint: Use Tutorial 9 and Groq Documentation\n",
        "# Explain each parameter and how each value change influences the LLM's output.\n",
        "# Prompt the model using the user role about anything different from the tutorial.\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides facts and reasoning.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Why do penguins not fly even though they are birds?\"}\n",
        "    ],\n",
        "    temperature=0.2,\n",
        "    top_p=0.7,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "# Print the assistant’s response\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2f2stNqCmP_"
      },
      "source": [
        "# Task 2: Agent Implementation (5 marks)\n",
        "\n",
        "This task contains an implementation of the agent from Tutorial 9. The idea of this task is to make sure you understand how basic LLM-Agent works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "feUcU4mCl2rn"
      },
      "outputs": [],
      "source": [
        "# Q2a: (5 marks) Explain how agent implementation works, providing comments line by line.\n",
        "# This paper might be helpful: https://react-lm.github.io/\n",
        "\n",
        "#This data class keeps track of what the agent has said and been told\n",
        "@dataclass\n",
        "class Agent_State:\n",
        "    messages: List[Dict[str, str]] #stores conversation messages between user and agent\n",
        "    system_prompt: str #sets the overall behavior and tone of the agent\n",
        "\n",
        "#The ML_Agent class is the core of our reasoning agent\n",
        "class ML_Agent:\n",
        "    def __init__(self, system_prompt: str):\n",
        "        #This sets up the client that talks to the LLM\n",
        "        self.client = client\n",
        "\n",
        "        #The agent’s memory starts with just the system prompt\n",
        "        self.state = Agent_State(\n",
        "            messages=[{\"role\": \"system\", \"content\": system_prompt}],\n",
        "            system_prompt=system_prompt,\n",
        "        )\n",
        "\n",
        "    def add_message(self, role: str, content: str) -> None:\n",
        "        #Add a new message to the conversation (user or assistant)\n",
        "        self.state.messages.append({\"role\": role, \"content\": content})\n",
        "\n",
        "    def execute(self) -> str:\n",
        "        #This sends all past messages to the LLM and gets the latest reply\n",
        "        completion = self.client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\", #the chosen LLM model\n",
        "            temperature=0.2, #low randomness (more predictable)\n",
        "            top_p=0.7, #controls how many token options are considered\n",
        "            max_tokens=1024, #maximum length of the response\n",
        "            messages=self.state.messages, #full conversation history\n",
        "        )\n",
        "        return completion.choices[0].message.content #return just the reply text\n",
        "\n",
        "    def __call__(self, message: str) -> str:\n",
        "        self.add_message(\"user\", message) #add the user's input\n",
        "        result = self.execute() #get the agent's response\n",
        "        self.add_message(\"assistant\", result) #save the agent's reply\n",
        "        return result #return the final answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3eOrZAElyrH"
      },
      "source": [
        "# Task 3: Tools (20 marks)\n",
        "\n",
        "Tools are specialized functions that enable AI agents to perform specific actions beyond their inherent capabilities, such as retrieving information, performing calculations, or manipulating data. Agents use tools to decompose complex reasoning into observable steps, extend their knowledge beyond training data, maintain state across interactions, and provide transparency in their decision-making process, ultimately allowing them to solve problems they couldn't tackle through reasoning alone.\n",
        "\n",
        "Essentially, tools are just callback functions invoked by the agent at the appropriate time during the execution loop.\n",
        "\n",
        "You need to plan your tools for each particular task your agent is expected to solve.\n",
        "The Model Evaluation Agent we are building should be able to evaluate the model from the model pool on the specific dataset.\n",
        "\n",
        "Datasets to use: Penguins, Iris, CIFAR-10\n",
        "\n",
        "You should be able to tell the agent what to do and watch it display the output of the tools' execution, similar to that in Tutorial 9.\n",
        "\n",
        "User Prompt examples you should be able to give to your agent and expect it to fulfill the task:\n",
        "- **Evaluate Linear Regression Model on Iris Dataset**\n",
        "- **Train a logistic regression model on the Iris dataset**\n",
        "- **Load the Penguins dataset and preprocess it.**\n",
        "- **Train a decision tree model on the Penguins dataset and evaluate it.**\n",
        "- **Load the CIFAR-10 dataset and train Mini-ResNet CNN, visualize results**\n",
        "\n",
        "Classifier Models for Iris and Penguins (use A1 and early tutorials):\n",
        "  * Logistic Regression (solver='lbfgs')\n",
        "  * Decision Tree (max_depth=3)\n",
        "  * KNN (n_neighbors=5)\n",
        "\n",
        "Any 2 CNN models of your choice for CIFAR-10 dataset (do some research, don't create anything from scratch unless you want to, use the ones provided by libraries and frameworks)\n",
        "\n",
        "HINT: It is highly recommended that any code from previous assignments and tutorials be reused for tool implementation.\n",
        "\n",
        "**Use Pytorch where possible**\n",
        "\n",
        "## DON'T FORGET TO IMPORT MISSING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PnqADgZqqPp9"
      },
      "outputs": [],
      "source": [
        "# Q3a (3 marks): Implement model_memory tool.\n",
        "# This tool should provide the agent with details about models or datasets\n",
        "# Example: when asked about Penguin dataset, the agent can use memory to look up\n",
        "# the source to obtain the dataset.\n",
        "\n",
        "\n",
        "def model_memory(query: str) -> str:\n",
        "    query = query.lower().strip()\n",
        "\n",
        "    #Predefined knowledge base\n",
        "    memory = {\n",
        "        \"iris dataset\": \"The Iris dataset contains 3 classes of 50 instances each, with 4 features (sepal/petal length/width).\",\n",
        "        \"penguins dataset\": \"The Penguins dataset includes 3 species of penguins with features like flipper length and body mass.\",\n",
        "        \"cifar-10 dataset\": \"CIFAR-10 contains 60,000 32x32 color images across 10 categories like airplane, dog, and truck.\",\n",
        "        \"logistic regression\": \"A linear classifier that uses the logistic function to model probabilities.\",\n",
        "        \"decision tree\": \"A tree-based model that splits data based on feature thresholds. Easy to interpret. Use max_depth=3 for simplicity.\",\n",
        "        \"knn\": \"K-Nearest Neighbors (KNN) classifies a sample based on the majority label of its k closest points. Use n_neighbors=5.\",\n",
        "        \"cnn\": \"Convolutional Neural Networks (CNNs) are deep learning models designed for visual recognition tasks like image classification.\"\n",
        "    }\n",
        "\n",
        "    return memory.get(query, f\"No known entry for '{query}'. Try asking about a dataset or model.\") #now return match or fallback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8sPHHfdVqPp9"
      },
      "outputs": [],
      "source": [
        "# Q3b (3 marks): Implement dataset_loader tool.\n",
        "# loads dataset after obtaining info from memory\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "import seaborn as sns\n",
        "\n",
        "def dataset_loader(dataset_name: str):\n",
        "    dataset_name = dataset_name.lower().strip()\n",
        "\n",
        "    if dataset_name == \"iris\":\n",
        "        data = load_iris(as_frame=True)\n",
        "        df = data.frame\n",
        "        return df.head().to_string() #preview only\n",
        "\n",
        "    elif dataset_name == \"penguins\":\n",
        "        df = sns.load_dataset(\"penguins\")\n",
        "        return df.head().to_string() #preview only\n",
        "\n",
        "    elif dataset_name == \"cifar-10\":\n",
        "        return \"CIFAR-10 will be loaded using torchvision.datasets when training starts (due to its size).\"\n",
        "\n",
        "    else:\n",
        "        return f\"Dataset '{dataset_name}' not recognized. Please choose Iris, Penguins, or CIFAR-10.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WNpzf3LSqPp9"
      },
      "outputs": [],
      "source": [
        "# Q3c (3 marks): Implement dataset_preprocessing tool.\n",
        "# preprocesses the dataset to work with the chosen model, and does the splits\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Global state so other tools (like training) can use the preprocessed data\n",
        "PREPROCESSED_DATA = {}\n",
        "\n",
        "def dataset_preprocessing(dataset_name: str) -> str:\n",
        "    dataset_name = dataset_name.lower().strip()\n",
        "\n",
        "    if dataset_name == \"iris\":\n",
        "        from sklearn.datasets import load_iris\n",
        "        data = load_iris(as_frame=True)\n",
        "        df = data.frame\n",
        "        X = df.drop(columns=[\"target\"])\n",
        "        y = df[\"target\"]\n",
        "\n",
        "    elif dataset_name == \"penguins\":\n",
        "        import seaborn as sns\n",
        "        df = sns.load_dataset(\"penguins\").dropna()\n",
        "        df = df.select_dtypes(include=[float, int])  # remove non-numerical columns like species\n",
        "        X = df.drop(columns=[\"body_mass_g\"]) if \"body_mass_g\" in df else df.iloc[:, :-1]\n",
        "        y = df[\"body_mass_g\"] if \"body_mass_g\" in df else df.iloc[:, -1]\n",
        "\n",
        "    else:\n",
        "        return f\"Preprocessing not supported for dataset '{dataset_name}'. Try 'iris' or 'penguins'.\"\n",
        "\n",
        "    #Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    #Split into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    #Save for other tools to use\n",
        "    PREPROCESSED_DATA[\"X_train\"] = X_train\n",
        "    PREPROCESSED_DATA[\"X_test\"] = X_test\n",
        "    PREPROCESSED_DATA[\"y_train\"] = y_train\n",
        "    PREPROCESSED_DATA[\"y_test\"] = y_test\n",
        "    PREPROCESSED_DATA[\"dataset_name\"] = dataset_name\n",
        "\n",
        "    return f\"{dataset_name.capitalize()} dataset preprocessed and split into train/test sets.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UdsyTSp_qPp9"
      },
      "outputs": [],
      "source": [
        "# Q3d (3 points): Implement train_model tool.\n",
        "# trains selected model on selected dataset, the agent should not use this tool\n",
        "# on datasets and models that cannot work together.\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#Store trained model so evaluation and visualization tools can access it\n",
        "TRAINED_MODEL = {}\n",
        "\n",
        "def train_model(model_name: str) -> str:\n",
        "    if \"X_train\" not in PREPROCESSED_DATA:\n",
        "        return \"Please preprocess the dataset first using dataset_preprocessing.\"\n",
        "\n",
        "    model_name = model_name.lower().strip()\n",
        "    dataset = PREPROCESSED_DATA.get(\"dataset_name\")\n",
        "\n",
        "    X_train = PREPROCESSED_DATA[\"X_train\"]\n",
        "    y_train = PREPROCESSED_DATA[\"y_train\"]\n",
        "\n",
        "    model = None\n",
        "\n",
        "    #Only allow supported model-dataset pairs\n",
        "    if dataset in [\"iris\", \"penguins\"]:\n",
        "        if model_name == \"logistic regression\":\n",
        "            model = LogisticRegression(solver=\"lbfgs\", max_iter=200)\n",
        "        elif model_name == \"decision tree\":\n",
        "            model = DecisionTreeClassifier(max_depth=3)\n",
        "        elif model_name == \"knn\":\n",
        "            model = KNeighborsClassifier(n_neighbors=5)\n",
        "        else:\n",
        "            return f\"Model '{model_name}' not supported for tabular datasets.\"\n",
        "\n",
        "    else:\n",
        "        return f\"Training not supported for dataset '{dataset}'. Use Iris or Penguins.\"\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    TRAINED_MODEL[\"model\"] = model #Store for later use\n",
        "    TRAINED_MODEL[\"model_name\"] = model_name\n",
        "\n",
        "    return f\"{model_name.title()} model trained successfully on the {dataset} dataset.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9PwGEcq3qPp9"
      },
      "outputs": [],
      "source": [
        "# Q3e (3 marks): Implement evaluate_model tool\n",
        "# evaluates the models and shows the quality metrics (accuracy, precision, and anything else of your choice)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "def evaluate_model(_: str = \"\") -> str:\n",
        "    if \"model\" not in TRAINED_MODEL or \"X_test\" not in PREPROCESSED_DATA:\n",
        "        return \"Please ensure both the model is trained and the dataset is preprocessed.\"\n",
        "\n",
        "    model = TRAINED_MODEL[\"model\"]\n",
        "    X_test = PREPROCESSED_DATA[\"X_test\"]\n",
        "    y_test = PREPROCESSED_DATA[\"y_test\"]\n",
        "\n",
        "    #Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    #Calculate key metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    #Return metrics as readable output\n",
        "    return (\n",
        "        f\"Evaluation Results:\\n\"\n",
        "        f\"Accuracy: {acc:.4f}\\n\"\n",
        "        f\"Precision: {precision:.4f}\\n\"\n",
        "        f\"Recall: {recall:.4f}\\n\"\n",
        "        f\"F1 Score: {f1:.4f}\\n\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TWdsndGrqPp9"
      },
      "outputs": [],
      "source": [
        "# Q3f (5 marks): Implement visualize_results tool\n",
        "# provides results of the training/evaluation, open-ended task (2 plots minimum)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def visualize_results(_: str = \"\") -> str:\n",
        "    if \"model\" not in TRAINED_MODEL or \"X_test\" not in PREPROCESSED_DATA:\n",
        "        return \"Please ensure both the model is trained and the dataset is preprocessed.\"\n",
        "\n",
        "    model = TRAINED_MODEL[\"model\"]\n",
        "    model_name = TRAINED_MODEL[\"model_name\"]\n",
        "    X_test = PREPROCESSED_DATA[\"X_test\"]\n",
        "    y_test = PREPROCESSED_DATA[\"y_test\"]\n",
        "    dataset = PREPROCESSED_DATA[\"dataset_name\"]\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    #Plot 1: Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f\"Confusion Matrix ({model_name.title()} on {dataset.title()})\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "    #Plot 2: Feature Importance (for Decision Tree or Logistic Regression)\n",
        "    if hasattr(model, \"coef_\"):\n",
        "        importances = np.abs(model.coef_).mean(axis=0)\n",
        "    elif hasattr(model, \"feature_importances_\"):\n",
        "        importances = model.feature_importances_\n",
        "    else:\n",
        "        return \"Second visualization not supported for this model type.\"\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.barplot(x=importances, y=[f\"Feature {i}\" for i in range(len(importances))])\n",
        "    plt.title(f\"Feature Importance ({model_name.title()})\")\n",
        "    plt.xlabel(\"Importance\")\n",
        "    plt.ylabel(\"Features\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return \"Visualizations generated successfully.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyY4lATzCmsf"
      },
      "source": [
        "# Task 4: System Prompt (10 marks)\n",
        "A system prompt is essential for guiding an agent's behavior by establishing its purpose, capabilities, tone, and workflow patterns. It acts as the \"personality and instruction manual\" for the agent, defining the format of interactions (like using Thought/Action/Observation steps in our ML agent), available tools, response styles, and domain-specific knowledge—all while remaining invisible to the end user. This hidden layer of instruction ensures the agent consistently follows the intended reasoning process and operational constraints while providing appropriate and helpful responses, effectively serving as the blueprint for the agent's behavior across all interactions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QCq6n5_FjJef"
      },
      "outputs": [],
      "source": [
        "# Q4a (10 marks) Build a system prompt to guide the agent based on Tutorial 9.\n",
        "# Use the following function:\n",
        "\n",
        "# Try to find alternative wording to keep the agent in the desired loop,\n",
        "# don't just copy the prompt from the tutorial.\n",
        "\n",
        "# Penalty for direct copy - 2 marks\n",
        "\n",
        "def create_agent():\n",
        "    system_prompt = \"\"\"\n",
        "You are a helpful AI agent built to assist with machine learning tasks using logical reasoning and a structured process.\n",
        "\n",
        "Follow this interaction format for every query:\n",
        "1. Think through the problem step-by-step and explain your thought process. Start this section with \"Thought:\".\n",
        "2. If needed, take an action by calling a tool. Format it as \"Action: tool_name: tool_input\".\n",
        "3. Wait for the result and respond to it with \"Observation:\".\n",
        "4. Repeat the Thought/Action/Observation cycle until enough information is gathered.\n",
        "5. Finish with a final answer or conclusion.\n",
        "\n",
        "Only use available tools when needed:\n",
        "- model_memory\n",
        "- dataset_loader\n",
        "- dataset_preprocessing\n",
        "- train_model\n",
        "- evaluate_model\n",
        "- visualize_results\n",
        "\n",
        "Only one action should be taken per step. Be methodical, and explain your decisions clearly to help the user understand each step.\n",
        "\n",
        "Do not skip steps. Wait for an observation before continuing. Stick to the cycle unless asked a direct fact-based question.\n",
        "    \"\"\".strip()\n",
        "\n",
        "    return ML_Agent(system_prompt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T16yokijI2P"
      },
      "source": [
        "# Task 5: Set the Agent Loop (10 marks)\n",
        "\n",
        "Now we are building automation of our Thought/Action/Observation sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "q82GuUEmcewk"
      },
      "outputs": [],
      "source": [
        "# Q5a: (2 marks) Explain why we need the following data structure and fill it in with appropriate values:\n",
        "KNOWN_ACTIONS = {\n",
        "    \"model_memory\": model_memory,               # Looks up information about a model or dataset\n",
        "    \"dataset_loader\": dataset_loader,           # Loads a dataset and returns a preview of its data\n",
        "    \"dataset_preprocessing\": dataset_preprocessing,  # Preprocesses the dataset and splits it into train/test sets\n",
        "    \"train_model\": train_model,                 # Trains a chosen model on the preprocessed data\n",
        "    \"evaluate_model\": evaluate_model,           # Evaluates the trained model using performance metrics\n",
        "    \"visualize_results\": visualize_results,     # Generates plots to visualize evaluation results\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "c7A5XTqrCnCf"
      },
      "outputs": [],
      "source": [
        "# Q5b: (6 marks) Explain how the agent automation loop works line by line. Why do we need the ACTION_PATTERN variable?\n",
        "# This paper might be helpful: https://react-lm.github.io/\n",
        "\n",
        "ACTION_PATTERN = re.compile(\"^Action: (\\w+): (.*)$\")\n",
        "\n",
        "number_of_steps = 5 # adjust this number for your implementation, to avoid an infinite loop\n",
        "\n",
        "def query(question: str, max_turns: int = number_of_steps) -> List[Dict[str, str]]:\n",
        "    agent = create_agent()\n",
        "    next_prompt = question\n",
        "\n",
        "    for turn in range(max_turns):\n",
        "        result = agent(next_prompt)\n",
        "        print(result)\n",
        "        actions = [\n",
        "            ACTION_PATTERN.match(a)\n",
        "            for a in result.split(\"\\n\")\n",
        "            if ACTION_PATTERN.match(a)\n",
        "        ]\n",
        "        if actions:\n",
        "            action, action_input = actions[0].groups()\n",
        "            if action not in KNOWN_ACTIONS:\n",
        "                raise ValueError(f\"Unknown action: {action}: {action_input}\")\n",
        "            print(f\"\\n ---> Executing {action} with input: {action_input}\")\n",
        "            observation = KNOWN_ACTIONS[action](action_input)\n",
        "            print(f\"Observation: {observation}\")\n",
        "            next_prompt = f\"Observation: {observation}\"\n",
        "        else:\n",
        "            break\n",
        "    return agent.state.messages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANSWER**\n",
        "\n",
        "The agent automation loop is coded to enable the AI to think step by step and decide what tool to invoke depending on the user question. It starts by creating a new agent with a system prompt and then sends the user's question to the new agent. Now the new agent we made gives its answer and maybe even an action like executing one of our tools we programmed. The loop checks if the response contains a call to a tool using the ACTION_PATTERN, which is a regular expression that identifies lines starting with \"Action: tool_name: input\". The pattern enables the code to recognize when the agent is requesting an action to be executed and input to use. If something is found and recognized, invoke the matching tool function and return its output to the agent as an \"Observation\" for the next step. Do this a few steps, where the agent can build a coherent chain of reasoning, use the tools when necessary, and finally return with a solution. This setup makes the agent interactive, modular, and able to process complicated tasks step by step."
      ],
      "metadata": {
        "id": "W6raSiag-VJX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "z33PNv77iwN_"
      },
      "outputs": [],
      "source": [
        "# Q5b: (2 marks)\n",
        "# QUESTION: How can we check the whole history of the agent's interaction with LLM?\n",
        "\n",
        "#We can look at the history of our agent's conversation with our LLM\n",
        "#by essentially looking through the messages stored in the agent's state.\n",
        "#These messages now basically contain everything we need,\n",
        "#such as what the system, user, or assistant said during our reasoning process.\n",
        "#Upon executing the query() function, the complete conversation history\n",
        "#is stored in agent.state.messages, which is also what the function returns.\n",
        "#Each message is a role-playing dictionary with a role such as a \"user\",\n",
        "#\"assistant\", \"system\", and the corresponding content.\n",
        "#This enables you to view the whole thought process, activities,\n",
        "#and results the agent followed step by step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8F2uGS_qPp-"
      },
      "source": [
        "# Task 6: Run your agent (15 marks)\n",
        "\n",
        "Let's see if your agent works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5tfBsrMqiwLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc8a5869-3ddd-4b77-8a6f-f633dd431458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example 1: Evaluate Linear Regression Model on Iris Dataset\n",
            "==================================================\n",
            "Thought: To evaluate a Linear Regression model on the Iris dataset, we first need to understand that Linear Regression is typically used for regression tasks, where the target variable is continuous. However, the Iris dataset is often used for classification tasks, as it aims to predict the species of iris flowers based on their characteristics, which is a categorical outcome. Despite this, we can still use Linear Regression for a multi-class classification problem by using one-vs-rest or similar approaches, but it's essential to note that other models like Logistic Regression or classifiers might be more suitable. Our first step should be to load the Iris dataset.\n",
            "\n",
            "Action: dataset_loader: iris\n",
            "\n",
            "Observation: \n",
            "\n",
            "(Please provide the observation after loading the dataset)\n",
            "\n",
            " ---> Executing dataset_loader with input: iris\n",
            "Observation:    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n",
            "0                5.1               3.5                1.4               0.2       0\n",
            "1                4.9               3.0                1.4               0.2       0\n",
            "2                4.7               3.2                1.3               0.2       0\n",
            "3                4.6               3.1                1.5               0.2       0\n",
            "4                5.0               3.6                1.4               0.2       0\n",
            "Thought: The dataset has been loaded, and we can see the first few rows of it. The dataset consists of 5 features: sepal length, sepal width, petal length, petal width, and a target variable that represents the species of iris flowers (0, 1, or 2, although only 0 is shown in the observation). Since Linear Regression is not the most suitable model for this classification problem, we might need to adjust our approach or compare its performance with more appropriate models. However, to proceed with the evaluation of a Linear Regression model as requested, we should preprocess the dataset. This might involve encoding the target variable, although for a linear regression model, we typically deal with continuous targets. For multi-class classification with linear regression, we might consider using a one-vs-rest approach, but first, let's focus on preprocessing the features.\n",
            "\n",
            "Action: dataset_preprocessing: iris\n",
            "\n",
            "Observation: \n",
            "\n",
            "(Please provide the observation after preprocessing the dataset)\n",
            "\n",
            " ---> Executing dataset_preprocessing with input: iris\n",
            "Observation: Iris dataset preprocessed and split into train/test sets.\n",
            "Thought: The dataset has been preprocessed and split into training and testing sets, which is a crucial step before training any model. This ensures that we have a separate set of data to evaluate the model's performance after training, preventing overfitting. Now, we can proceed to train a Linear Regression model on the training set. Although Linear Regression is typically used for regression tasks, we can still use it for our multi-class classification problem by using a one-vs-rest approach or similar. However, it's worth noting that the model's performance might not be optimal compared to models specifically designed for classification tasks.\n",
            "\n",
            "Action: train_model: Linear Regression, iris_train\n",
            "\n",
            "Observation: \n",
            "\n",
            "(Please provide the observation after training the model)\n",
            "\n",
            " ---> Executing train_model with input: Linear Regression, iris_train\n",
            "Observation: Model 'linear regression, iris_train' not supported for tabular datasets.\n",
            "Thought: The observation indicates that the Linear Regression model is not supported for tabular datasets like the Iris dataset in the context of this task. This could be due to the nature of the dataset or the specific requirements of the tools being used. Given that Linear Regression is generally applicable to tabular data, this might be a limitation of the current setup or tools. In a typical scenario, Linear Regression can be applied to tabular datasets, but it seems we need to adjust our approach here. \n",
            "\n",
            "Considering the classification nature of the Iris dataset, a more suitable model would be one that is designed for classification tasks, such as Logistic Regression. However, since the task specifically asks for the evaluation of a Linear Regression model, we might need to reconsider how to apply or simulate a Linear Regression approach for a classification problem, possibly by using it in a one-vs-rest manner for multi-class classification. But given the tool's limitation, we might need to directly evaluate the model's performance using a different method or accept that Linear Regression might not be directly applicable here.\n",
            "\n",
            "Action: evaluate_model: Linear Regression, iris_test\n",
            "\n",
            "Observation: \n",
            "\n",
            "(Please provide the observation after attempting to evaluate the model)\n",
            "\n",
            " ---> Executing evaluate_model with input: Linear Regression, iris_test\n",
            "Observation: Please ensure both the model is trained and the dataset is preprocessed.\n",
            "Thought: The observation suggests that the model needs to be trained and the dataset preprocessed before evaluation. However, we've already attempted to train the model and preprocess the dataset. The issue seems to be that the Linear Regression model is not supported for the Iris dataset as per the previous observation. Given this, we should reconsider our approach and potentially use a different model that is supported for classification tasks on the Iris dataset.\n",
            "\n",
            "Since we cannot directly use Linear Regression due to the tool's limitations and the nature of the dataset, we should acknowledge that evaluating a Linear Regression model on the Iris dataset is not feasible with the current setup. Instead, we could discuss the theoretical aspects of how one might approach using Linear Regression for a classification task, but in practice, we would recommend using a more suitable model for classification, such as Logistic Regression or other classifiers.\n",
            "\n",
            "Given the constraints and the goal to follow the instructions precisely, we must conclude that we cannot proceed with the evaluation as initially planned due to the tool's limitations and the mismatch between the model and the dataset's task.\n",
            "\n",
            "Conclusion: Due to the tool's limitations and the nature of the Iris dataset, evaluating a Linear Regression model as requested is not feasible. For classification tasks on the Iris dataset, models like Logistic Regression or other classifiers would be more appropriate.\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "Example 1: Train a logistic regression model on the Iris dataset\n",
            "==================================================\n",
            "Thought: To train a logistic regression model on the Iris dataset, we first need to load the dataset. The Iris dataset is a classic multiclass classification problem, where we have 50 samples from each of three species of Iris flowers (Iris setosa, Iris virginica, and Iris versicolor). Each sample is described by 4 features: the length and the width of the sepals and petals. Since logistic regression is typically used for binary classification, we might need to use a one-vs-all or one-vs-one approach for multiclass classification. However, many implementations of logistic regression in machine learning libraries can handle multiclass problems directly.\n",
            "\n",
            "Action: dataset_loader: Iris\n",
            "\n",
            "Observation:\n",
            "\n",
            " ---> Executing dataset_loader with input: Iris\n",
            "Observation:    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n",
            "0                5.1               3.5                1.4               0.2       0\n",
            "1                4.9               3.0                1.4               0.2       0\n",
            "2                4.7               3.2                1.3               0.2       0\n",
            "3                4.6               3.1                1.5               0.2       0\n",
            "4                5.0               3.6                1.4               0.2       0\n",
            "Thought: The dataset has been loaded, and we can see the first few rows of the dataset. The dataset contains 5 columns: sepal length, sepal width, petal length, petal width, and the target variable, which represents the species of Iris flowers (0, 1, or 2). The next step is to preprocess the dataset. Since logistic regression can handle numerical data directly, we might not need to perform any significant transformations. However, it's essential to split the dataset into training and testing sets to evaluate the model's performance.\n",
            "\n",
            "Action: dataset_preprocessing: split Iris dataset into training and testing sets\n",
            "\n",
            "Observation:\n",
            "\n",
            " ---> Executing dataset_preprocessing with input: split Iris dataset into training and testing sets\n",
            "Observation: Preprocessing not supported for dataset 'split iris dataset into training and testing sets'. Try 'iris' or 'penguins'.\n",
            "Thought: It seems that the dataset_preprocessing tool does not support splitting a dataset into training and testing sets directly. However, we can still use the dataset_preprocessing tool to load and preprocess the Iris dataset. Then, we can assume that the tool will split the dataset into training and testing sets internally or provide a pre-split dataset.\n",
            "\n",
            "Action: dataset_preprocessing: iris\n",
            "\n",
            "Observation:\n",
            "\n",
            " ---> Executing dataset_preprocessing with input: iris\n",
            "Observation: Iris dataset preprocessed and split into train/test sets.\n",
            "Thought: The Iris dataset has been preprocessed and split into training and testing sets. Now, we can proceed with training a logistic regression model on the preprocessed dataset. We will use the train_model tool to train the model, specifying the type of model as logistic regression and the dataset as the preprocessed Iris dataset.\n",
            "\n",
            "Action: train_model: logistic_regression iris\n",
            "\n",
            "Observation:\n",
            "\n",
            " ---> Executing train_model with input: logistic_regression iris\n",
            "Observation: Model 'logistic_regression iris' not supported for tabular datasets.\n",
            "Thought: It seems that the train_model tool does not support training a logistic regression model directly on the Iris dataset. However, we can try training a logistic regression model on the preprocessed Iris dataset without specifying the dataset name. This might allow the tool to recognize the dataset and train the model accordingly.\n",
            "\n",
            "Action: train_model: logistic_regression\n",
            "\n",
            "Observation:\n",
            "\n",
            " ---> Executing train_model with input: logistic_regression\n",
            "Observation: Model 'logistic_regression' not supported for tabular datasets.\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "Example 2: Load the Penguins dataset and preprocess it.\n",
            "==================================================\n",
            "Thought: To load the Penguins dataset and preprocess it, we first need to ensure that the dataset is available and can be loaded into our system. The Penguins dataset is a commonly used dataset for machine learning tasks, particularly for classification and regression problems. After loading the dataset, we will need to preprocess it, which may involve handling missing values, encoding categorical variables, and scaling numerical variables.\n",
            "\n",
            "Action: dataset_loader: Penguins\n",
            "\n",
            "Observation:\n",
            "\n",
            " ---> Executing dataset_loader with input: Penguins\n",
            "Observation:   species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g     sex\n",
            "0  Adelie  Torgersen            39.1           18.7              181.0       3750.0    Male\n",
            "1  Adelie  Torgersen            39.5           17.4              186.0       3800.0  Female\n",
            "2  Adelie  Torgersen            40.3           18.0              195.0       3250.0  Female\n",
            "3  Adelie  Torgersen             NaN            NaN                NaN          NaN     NaN\n",
            "4  Adelie  Torgersen            36.7           19.3              193.0       3450.0  Female\n",
            "Thought: The dataset has been loaded, and we can see that it contains various features such as species, island, bill length, bill depth, flipper length, body mass, and sex. However, there are missing values in the dataset, as indicated by the NaN values in row 3. To effectively use this dataset for machine learning tasks, we need to preprocess it to handle these missing values and potentially scale or encode the features.\n",
            "\n",
            "Action: dataset_preprocessing: Penguins\n",
            "\n",
            "Observation:\n",
            "\n",
            " ---> Executing dataset_preprocessing with input: Penguins\n",
            "Observation: Penguins dataset preprocessed and split into train/test sets.\n",
            "Thought: The dataset has been preprocessed, which likely involved handling the missing values and potentially scaling or encoding the features. The dataset has also been split into training and testing sets, which is a crucial step in evaluating the performance of a machine learning model. The training set will be used to train the model, while the testing set will be used to evaluate its performance on unseen data. Now that the data is ready, we can proceed with training a model.\n",
            "\n",
            "Action: train_model: Penguins_train\n",
            "\n",
            "Observation:\n",
            "\n",
            " ---> Executing train_model with input: Penguins_train\n",
            "Observation: Model 'penguins_train' not supported for tabular datasets.\n",
            "Thought: It seems that the model I attempted to train is not compatible with tabular datasets like the Penguins dataset. This could be due to the model being designed for image or text data, or it might require a specific type of input. For tabular datasets, we often use models like decision trees, random forests, or logistic regression. We need to select a suitable model that can handle the preprocessed Penguins dataset.\n",
            "\n",
            "Action: train_model: Penguins_train, model='RandomForestClassifier'\n",
            "\n",
            "Observation:\n",
            "\n",
            " ---> Executing train_model with input: Penguins_train, model='RandomForestClassifier'\n",
            "Observation: Model 'penguins_train, model='randomforestclassifier'' not supported for tabular datasets.\n",
            "Thought: It seems that the model specification I provided is still not compatible with the tabular dataset. The issue might be with the way the model is specified or the tool's requirements for training a model. Since we have a preprocessed dataset ready, we should try a more straightforward approach to train a model. Let's try training a model without specifying the model type, allowing the system to choose a suitable default model for the tabular dataset.\n",
            "\n",
            "Action: train_model: Penguins_train\n",
            "\n",
            "Observation:\n",
            "\n",
            " ---> Executing train_model with input: Penguins_train\n",
            "Observation: Model 'penguins_train' not supported for tabular datasets.\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "Example 3: Train a decision tree model on the Penguins dataset and evaluate it.\n",
            "==================================================\n",
            "Thought: To train a decision tree model on the Penguins dataset, we first need to load the dataset. The Penguins dataset is a multiclass classification problem, where we predict the species of penguins based on their physical characteristics. We should start by loading the dataset to understand its structure and content.\n",
            "\n",
            "Action: dataset_loader: Penguins\n",
            "\n",
            "Observation: \n",
            "\n",
            "(Please provide the output of the dataset_loader tool)\n",
            "\n",
            "Thought: Once we have loaded the dataset, we can proceed with preprocessing it to ensure it's in a suitable format for training a decision tree model. This might involve handling missing values, encoding categorical variables, and scaling/normalizing numerical features.\n",
            "\n",
            "Action: dataset_preprocessing: Penguins\n",
            "\n",
            "Observation: \n",
            "\n",
            "(Please provide the output of the dataset_preprocessing tool)\n",
            "\n",
            "Thought: With the dataset preprocessed, we can now train a decision tree model. We'll use the preprocessed dataset as input for the model training process.\n",
            "\n",
            "Action: train_model: decision_tree, Penguins_preprocessed\n",
            "\n",
            "Observation: \n",
            "\n",
            "(Please provide the output of the train_model tool)\n",
            "\n",
            "Thought: After training the decision tree model, we need to evaluate its performance to understand how well it generalizes to unseen data. This involves using metrics such as accuracy, precision, recall, and F1 score.\n",
            "\n",
            "Action: evaluate_model: decision_tree, Penguins_preprocessed\n",
            "\n",
            "Observation: \n",
            "\n",
            "(Please provide the output of the evaluate_model tool)\n",
            "\n",
            "Thought: Finally, to better understand the model's performance and identify any patterns or trends in the predictions, we can visualize the results. This could involve plotting a confusion matrix or using dimensionality reduction techniques to visualize the data and predictions.\n",
            "\n",
            "Action: visualize_results: decision_tree, Penguins_preprocessed\n",
            "\n",
            "Observation: \n",
            "\n",
            "(Please provide the output of the visualize_results tool)\n",
            "\n",
            "Thought: Based on the evaluation metrics and visualized results, we can conclude the performance of the decision tree model on the Penguins dataset.\n",
            "\n",
            "(Final answer will be provided after all observations)\n",
            "\n",
            " ---> Executing dataset_loader with input: Penguins\n",
            "Observation:   species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g     sex\n",
            "0  Adelie  Torgersen            39.1           18.7              181.0       3750.0    Male\n",
            "1  Adelie  Torgersen            39.5           17.4              186.0       3800.0  Female\n",
            "2  Adelie  Torgersen            40.3           18.0              195.0       3250.0  Female\n",
            "3  Adelie  Torgersen             NaN            NaN                NaN          NaN     NaN\n",
            "4  Adelie  Torgersen            36.7           19.3              193.0       3450.0  Female\n",
            "Thought: The observation shows the first few rows of the Penguins dataset, which includes features such as species, island, bill length, bill depth, flipper length, body mass, and sex. We notice that there is a row with missing values (NaN) for some of the features, which will need to be handled during the preprocessing step. The presence of categorical variables like species, island, and sex also suggests that we may need to perform encoding to convert them into numerical variables that can be used by the decision tree model.\n",
            "\n",
            "Action: dataset_preprocessing: Penguins\n",
            "\n",
            "Observation: \n",
            "\n",
            "(Please provide the output of the dataset_preprocessing tool, which should include information about how the missing values were handled and how the categorical variables were encoded)\n",
            "\n",
            " ---> Executing dataset_preprocessing with input: Penguins\n",
            "Observation: Penguins dataset preprocessed and split into train/test sets.\n",
            "Thought: Now that the dataset has been preprocessed and split into training and testing sets, we can proceed with training a decision tree model on the training data. This will involve using the preprocessed features to learn the relationships between the input variables and the target variable (species). We'll use the training set to fit the model and then evaluate its performance on the testing set.\n",
            "\n",
            "Action: train_model: decision_tree, Penguins_preprocessed\n",
            "\n",
            "Observation: \n",
            "\n",
            "(Please provide the output of the train_model tool, which should include information about the trained model, such as its parameters and any relevant metrics)\n",
            "\n",
            " ---> Executing train_model with input: decision_tree, Penguins_preprocessed\n",
            "Observation: Model 'decision_tree, penguins_preprocessed' not supported for tabular datasets.\n",
            "Thought: It seems that the decision tree model I specified earlier is not compatible with the preprocessed Penguins dataset. This could be due to the specific implementation of the model or the format of the dataset. To resolve this issue, we can try using a different model that is supported for tabular datasets, such as a random forest classifier, or we can try to modify the decision tree model to work with the preprocessed dataset.\n",
            "\n",
            "Action: train_model: random_forest, Penguins_preprocessed\n",
            "\n",
            "Observation: \n",
            "\n",
            "(Please provide the output of the train_model tool, which should indicate whether the random forest model was successfully trained on the preprocessed dataset)\n",
            "\n",
            " ---> Executing train_model with input: random_forest, Penguins_preprocessed\n",
            "Observation: Model 'random_forest, penguins_preprocessed' not supported for tabular datasets.\n",
            "Thought: It seems that the random forest model is also not compatible with the preprocessed Penguins dataset. This suggests that the issue may not be with the specific model, but rather with the way the dataset is being handled. We can try to use a different approach to train a model on the dataset. Since the dataset is tabular, we can try to use a model that is specifically designed for tabular data, such as a model that uses the dataset_loader and train_model tools in a more general way.\n",
            "\n",
            "Action: train_model: default, Penguins_preprocessed\n",
            "\n",
            "Observation: \n",
            "\n",
            "(Please provide the output of the train_model tool, which should indicate whether a default model was successfully trained on the preprocessed dataset)\n",
            "\n",
            " ---> Executing train_model with input: default, Penguins_preprocessed\n",
            "Observation: Model 'default, penguins_preprocessed' not supported for tabular datasets.\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Execute any THREE example prompts using your agent. (Each working prompt exaple will give you 5 marks, 5x3=15)\n",
        "# DONT FORGET TO SAVE THE OUTPUT\n",
        "\n",
        "# User Prompt examples you should be able to give to your agent:\n",
        "# **Evaluate Linear Regression Model on Iris Dataset**\n",
        "# **Train a logistic regression model on the Iris dataset**\n",
        "# **Load the Penguins dataset and preprocess it.**\n",
        "# **Train a decision tree model on the Penguins dataset and evaluate it.**\n",
        "# **Load the CIFAR-10 dataset and train Mini-ResNet CNN, visualize results**\n",
        "\n",
        "# Use this template:\n",
        "\n",
        "# Example 1: Prompt\n",
        "print(\"\\nExample 1: Evaluate Linear Regression Model on Iris Dataset\")\n",
        "print(\"=\" * 50)\n",
        "task = \"Evaluate Linear Regression Model on Iris Dataset\"\n",
        "result1 = query(task)\n",
        "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "# Example 2: Prompt\n",
        "print(\"\\nExample 1: Train a logistic regression model on the Iris dataset\")\n",
        "print(\"=\" * 50)\n",
        "task = \"Train a logistic regression model on the Iris dataset\"\n",
        "result2 = query(task)\n",
        "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "# Example 3: Prompt\n",
        "print(\"\\nExample 2: Load the Penguins dataset and preprocess it.\")\n",
        "print(\"=\" * 50)\n",
        "task = \"Load the Penguins dataset and preprocess it.\"\n",
        "result3 = query(task)\n",
        "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "# Example 4: Prompt\n",
        "print(\"\\nExample 3: Train a decision tree model on the Penguins dataset and evaluate it.\")\n",
        "print(\"=\" * 50)\n",
        "task = \"Train a decision tree model on the Penguins dataset and evaluate it.\"\n",
        "result4 = query(task)\n",
        "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIOy4EGr02D4"
      },
      "source": [
        "# Task 7: BONUS (10 points)\n",
        "Not valid without completion of all the previous tasks and tool implementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "10N4ZEGjiwIv"
      },
      "outputs": [],
      "source": [
        "# Build your own additional ML-related tool and provide an example of interaction with your reasoning agent\n",
        "# using a prompt of your choice that makes the agent use your tool at one of the reasoning steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG9_BGQrG1go"
      },
      "source": [
        "Good luck!\n",
        "\n",
        "## Signature:\n",
        "Don't forget to insert your name and student number and execute the snippet below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AKSDTADVqPp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4096a5-8c42-4de4-b99e-c092e53ccdaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: watermark in /usr/local/lib/python3.11/dist-packages (2.5.0)\n",
            "Requirement already satisfied: ipython>=6.0 in /usr/local/lib/python3.11/dist-packages (from watermark) (7.34.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from watermark) (8.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from watermark) (75.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->watermark) (3.21.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.0->watermark) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.0->watermark) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.0->watermark) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.0->watermark) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.0->watermark) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.0->watermark) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.0->watermark) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.0->watermark) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.0->watermark) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.0->watermark) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.0->watermark) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.0->watermark) (0.2.13)\n",
            "The watermark extension is already loaded. To reload it, use:\n",
            "  %reload_ext watermark\n",
            "Author: Bisher Abou-Alwan, #101211242, Mohammad Abusalem, #101204665\n",
            "\n",
            "Python implementation: CPython\n",
            "Python version       : 3.11.11\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "numpy     : 2.0.2\n",
            "pandas    : 2.2.2\n",
            "sklearn   : 1.6.1\n",
            "matplotlib: 3.10.0\n",
            "seaborn   : 0.13.2\n",
            "graphviz  : 0.20.3\n",
            "groq      : 0.22.0\n",
            "torch     : 2.6.0+cu124\n",
            "\n",
            "Compiler    : GCC 11.4.0\n",
            "OS          : Linux\n",
            "Release     : 6.1.85+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install watermark\n",
        "# Provide your Signature:\n",
        "%load_ext watermark\n",
        "%watermark -a 'Bisher Abou-Alwan, #101211242, Mohammad Abusalem, #101204665' -nmv --packages numpy,pandas,sklearn,matplotlib,seaborn,graphviz,groq,torch"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}