{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bisheralwan/ChatBot/blob/main/W2025/Assignments/A3/SYSC4415_W25_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcaEf9LdAu9k"
      },
      "source": [
        "# Welcome to Assignment 3\n",
        "\n",
        "**TA: [Igor Bogdanov](mailto:igorbogdanov@cmail.carleton.ca)**\n",
        "\n",
        "## General Instructions:\n",
        "\n",
        "This Assignment can be done **in a group of two or individually**.\n",
        "\n",
        "YOU HAVE TO JOIN A GROUP ON BRIGHTSPACE TO SUBMIT.\n",
        "\n",
        "Please state it explicitly at the beginning of the assignment.\n",
        "\n",
        "You need only one submission if it's group work.\n",
        "\n",
        "Please print out values when asked using Python's print() function with f-strings where possible.\n",
        "\n",
        "Submit your **saved notebook with all the outputs** to Brightspace, but ensure it will produce correct outputs upon restarting and click \"runtime\" → \"run all\" with clean outputs. Ensure your notebook displays all answers correctly.\n",
        "\n",
        "## Your Submission MUST contain your signature at the bottom.\n",
        "\n",
        "### Objective:\n",
        "In this assignment, we build a reasoning AI agent that facilitates ML operations and model evaluation. This assignment is heavily based on Tutorial 9.\n",
        "\n",
        "**Submission:** Submit your Notebook as a *.ipynb* file that adopts this naming convention: ***SYSC4415_W25_A3_NameLastname.ipynb*** on *Brightspace*. No other submission (e.g., through email) will be accepted. (Example file name: SYSC4415_W25_A3_IgorBogdanov.ipynb or SYSC4415_W25_A3_Student1_Student2.ipynb) The notebool MUST contain saved outputs\n",
        "\n",
        "**Runtime tips:**\n",
        "Agentic programming and API calling can be easily done locally and moved to Colab in the final stages, depending on the implementation of your tools and ML tasks you want to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIzWURYdCos_"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyRG5AEHNILq"
      },
      "source": [
        "Some basic libraries you need are imported here. Make sure you include whatever library you need in this entire notebook in the code block below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXYKklNMNpbQ"
      },
      "source": [
        "If you are using any library that requires installation, please paste the installation command here.\n",
        "Leave the code block below if you are not installing any libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZEeRARqqPp8"
      },
      "outputs": [],
      "source": [
        "# Name:\n",
        "# Student Number:\n",
        "\n",
        "# Name:\n",
        "# Student Number:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrxZ_P9tNkln"
      },
      "outputs": [],
      "source": [
        "# Libraries to install - leave this code block blank if this does not apply to you\n",
        "# Please add a brief comment on why you need the library and what it does\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3_N_y3sNfTC",
        "outputId": "ebb28c4c-7f84-4c1a-f930-401410b435d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n",
        "\n",
        "# Libraries you might need\n",
        "# General\n",
        "import os\n",
        "import zipfile\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# For pre-processing\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# For modeling\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import torchsummary\n",
        "\n",
        "# For metrics\n",
        "from sklearn.metrics import  accuracy_score\n",
        "from sklearn.metrics import  precision_score\n",
        "from sklearn.metrics import  recall_score\n",
        "from sklearn.metrics import  f1_score\n",
        "from sklearn.metrics import  classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import  roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Agent\n",
        "from groq import Groq\n",
        "from dataclasses import dataclass\n",
        "import re\n",
        "from typing import Dict, List, Optional\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vX4Z_nNI6BY"
      },
      "source": [
        "# Task 1: Registration and API Activation (5 marks)\n",
        "\n",
        "For this particular assignment, we will be using GroqCloud for LLM inference. This task aims to determine how to use the Groq API with LLMs.  \n",
        "\n",
        "Create a free account on https://groq.com/ and generate an API Key. Don't remove your key until you get your grade. Feel free to delete your API key after the term is completed.\n",
        "\n",
        "In conversational AI, prompting involves three key roles: the system role (which sets the agent's behavior and capabilities), the user role (which represents human inputs and queries), and the assistant role (which contains the agent's responses). The system role provides the foundational instructions and constraints, the user role delivers the actual queries or commands, and the assistant role generates contextual, step-by-step responses following the system's guidelines. This structured approach ensures consistent, controlled interactions where the agent maintains its defined behavior while responding to user needs, with each role serving a specific purpose in the conversation flow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tSc4kOsghn_"
      },
      "outputs": [],
      "source": [
        "# Q1a (2 mark)\n",
        "# Create a client using your API key.\n",
        "\n",
        "client = Groq(api_key=\"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\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT-vxP0QI74n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df2e87f-a695-4f69-c1f4-c13f245ade33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Penguins are indeed birds, but they have evolved to become flightless over time. There are several reasons for this:\n",
            "\n",
            "1. **Environmental pressures**: Penguins live in the Southern Hemisphere, where the climate is cold and the oceans are rich in food. As a result, they didn't need to fly to find food or escape predators. In fact, flying would have been energetically expensive and potentially hazardous in the strong winds and icy conditions.\n",
            "2. **Body shape and size**: Penguins have a unique body shape that is adapted for swimming and diving, rather than flying. They have a streamlined body, flippers instead of wings, and a heavy skeleton that helps them dive deep into the water. Their wings, although modified, are still present but are much shorter and more rigid than those of flying birds.\n",
            "3. **Energy efficiency**: Flying is a highly energy-intensive activity, requiring powerful muscles, lightweight bones, and a high metabolic rate. Penguins, on the other hand, have evolved to be highly efficient swimmers, using their flippers to propel themselves through the water with minimal energy expenditure. This allows them to conserve energy for other activities, such as breeding, molting, and escaping predators.\n",
            "4. **Evolutionary trade-offs**: As penguins evolved to become more specialized for aquatic life, they likely faced trade-offs between flying and swimming abilities. For example, the energy and resources required to maintain flight capabilities might have been redirected towards developing stronger flippers, more efficient swimming muscles, and a more streamlined body.\n",
            "5. **Phylogenetic history**: Penguins are thought to have evolved from flying ancestors around 60-80 million years ago, during the Cretaceous period. Over time, their flying abilities were likely lost as they adapted to their aquatic environment and developed new specializations.\n",
            "\n",
            "In summary, penguins don't fly because their evolutionary history, body shape, and environmental pressures have led them to become highly specialized for swimming and diving, rather than flying. Their unique adaptations have allowed them to thrive in their aquatic environment, making flying unnecessary for their survival.\n"
          ]
        }
      ],
      "source": [
        "# Q1b (3 marks)\n",
        "\n",
        "# instantiate chat_completion object using model of your choice (llama-3.3-70b-versatile - recommended)\n",
        "# Hint: Use Tutorial 9 and Groq Documentation\n",
        "# Explain each parameter and how each value change influences the LLM's output.\n",
        "# Prompt the model using the user role about anything different from the tutorial.\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides facts and reasoning.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Why do penguins not fly even though they are birds?\"}\n",
        "    ],\n",
        "    temperature=0.2,\n",
        "    top_p=0.7,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "# Print the assistant’s response\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2f2stNqCmP_"
      },
      "source": [
        "# Task 2: Agent Implementation (5 marks)\n",
        "\n",
        "This task contains an implementation of the agent from Tutorial 9. The idea of this task is to make sure you understand how basic LLM-Agent works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feUcU4mCl2rn"
      },
      "outputs": [],
      "source": [
        "# Q2a: (5 marks) Explain how agent implementation works, providing comments line by line.\n",
        "# This paper might be helpful: https://react-lm.github.io/\n",
        "\n",
        "#This data class keeps track of what the agent has said and been told\n",
        "@dataclass\n",
        "class Agent_State:\n",
        "    messages: List[Dict[str, str]] #stores conversation messages between user and agent\n",
        "    system_prompt: str #sets the overall behavior and tone of the agent\n",
        "\n",
        "#The ML_Agent class is the core of our reasoning agent\n",
        "class ML_Agent:\n",
        "    def __init__(self, system_prompt: str):\n",
        "        #This sets up the client that talks to the LLM\n",
        "        self.client = client\n",
        "\n",
        "        #The agent’s memory starts with just the system prompt\n",
        "        self.state = Agent_State(\n",
        "            messages=[{\"role\": \"system\", \"content\": system_prompt}],\n",
        "            system_prompt=system_prompt,\n",
        "        )\n",
        "\n",
        "    def add_message(self, role: str, content: str) -> None:\n",
        "        #Add a new message to the conversation (user or assistant)\n",
        "        self.state.messages.append({\"role\": role, \"content\": content})\n",
        "\n",
        "    def execute(self) -> str:\n",
        "        #This sends all past messages to the LLM and gets the latest reply\n",
        "        completion = self.client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\", #the chosen LLM model\n",
        "            temperature=0.2, #low randomness (more predictable)\n",
        "            top_p=0.7, #controls how many token options are considered\n",
        "            max_tokens=1024, #maximum length of the response\n",
        "            messages=self.state.messages, #full conversation history\n",
        "        )\n",
        "        return completion.choices[0].message.content #return just the reply text\n",
        "\n",
        "    def __call__(self, message: str) -> str:\n",
        "        self.add_message(\"user\", message) #add the user's input\n",
        "        result = self.execute() #get the agent's response\n",
        "        self.add_message(\"assistant\", result) #save the agent's reply\n",
        "        return result #return the final answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3eOrZAElyrH"
      },
      "source": [
        "# Task 3: Tools (20 marks)\n",
        "\n",
        "Tools are specialized functions that enable AI agents to perform specific actions beyond their inherent capabilities, such as retrieving information, performing calculations, or manipulating data. Agents use tools to decompose complex reasoning into observable steps, extend their knowledge beyond training data, maintain state across interactions, and provide transparency in their decision-making process, ultimately allowing them to solve problems they couldn't tackle through reasoning alone.\n",
        "\n",
        "Essentially, tools are just callback functions invoked by the agent at the appropriate time during the execution loop.\n",
        "\n",
        "You need to plan your tools for each particular task your agent is expected to solve.\n",
        "The Model Evaluation Agent we are building should be able to evaluate the model from the model pool on the specific dataset.\n",
        "\n",
        "Datasets to use: Penguins, Iris, CIFAR-10\n",
        "\n",
        "You should be able to tell the agent what to do and watch it display the output of the tools' execution, similar to that in Tutorial 9.\n",
        "\n",
        "User Prompt examples you should be able to give to your agent and expect it to fulfill the task:\n",
        "- **Evaluate Linear Regression Model on Iris Dataset**\n",
        "- **Train a logistic regression model on the Iris dataset**\n",
        "- **Load the Penguins dataset and preprocess it.**\n",
        "- **Train a decision tree model on the Penguins dataset and evaluate it.**\n",
        "- **Load the CIFAR-10 dataset and train Mini-ResNet CNN, visualize results**\n",
        "\n",
        "Classifier Models for Iris and Penguins (use A1 and early tutorials):\n",
        "  * Logistic Regression (solver='lbfgs')\n",
        "  * Decision Tree (max_depth=3)\n",
        "  * KNN (n_neighbors=5)\n",
        "\n",
        "Any 2 CNN models of your choice for CIFAR-10 dataset (do some research, don't create anything from scratch unless you want to, use the ones provided by libraries and frameworks)\n",
        "\n",
        "HINT: It is highly recommended that any code from previous assignments and tutorials be reused for tool implementation.\n",
        "\n",
        "**Use Pytorch where possible**\n",
        "\n",
        "## DON'T FORGET TO IMPORT MISSING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnqADgZqqPp9"
      },
      "outputs": [],
      "source": [
        "# Q3a (3 marks): Implement model_memory tool.\n",
        "# This tool should provide the agent with details about models or datasets\n",
        "# Example: when asked about Penguin dataset, the agent can use memory to look up\n",
        "# the source to obtain the dataset.\n",
        "\n",
        "\n",
        "def model_memory(query: str) -> str:\n",
        "    query = query.lower().strip()\n",
        "\n",
        "    #Predefined knowledge base\n",
        "    memory = {\n",
        "        \"iris dataset\": \"The Iris dataset contains 3 classes of 50 instances each, with 4 features (sepal/petal length/width).\",\n",
        "        \"penguins dataset\": \"The Penguins dataset includes 3 species of penguins with features like flipper length and body mass.\",\n",
        "        \"cifar-10 dataset\": \"CIFAR-10 contains 60,000 32x32 color images across 10 categories like airplane, dog, and truck.\",\n",
        "        \"logistic regression\": \"A linear classifier that uses the logistic function to model probabilities.\",\n",
        "        \"decision tree\": \"A tree-based model that splits data based on feature thresholds. Easy to interpret. Use max_depth=3 for simplicity.\",\n",
        "        \"knn\": \"K-Nearest Neighbors (KNN) classifies a sample based on the majority label of its k closest points. Use n_neighbors=5.\",\n",
        "        \"cnn\": \"Convolutional Neural Networks (CNNs) are deep learning models designed for visual recognition tasks like image classification.\"\n",
        "    }\n",
        "\n",
        "    return memory.get(query, f\"No known entry for '{query}'. Try asking about a dataset or model.\") #now return match or fallback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sPHHfdVqPp9"
      },
      "outputs": [],
      "source": [
        "# Q3b (3 marks): Implement dataset_loader tool.\n",
        "# loads dataset after obtaining info from memory\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "import seaborn as sns\n",
        "\n",
        "def dataset_loader(dataset_name: str):\n",
        "    dataset_name = dataset_name.lower().strip()\n",
        "\n",
        "    if dataset_name == \"iris\":\n",
        "        data = load_iris(as_frame=True)\n",
        "        df = data.frame\n",
        "        return df.head().to_string() #preview only\n",
        "\n",
        "    elif dataset_name == \"penguins\":\n",
        "        df = sns.load_dataset(\"penguins\")\n",
        "        return df.head().to_string() #preview only\n",
        "\n",
        "    elif dataset_name == \"cifar-10\":\n",
        "        return \"CIFAR-10 will be loaded using torchvision.datasets when training starts (due to its size).\"\n",
        "\n",
        "    else:\n",
        "        return f\"Dataset '{dataset_name}' not recognized. Please choose Iris, Penguins, or CIFAR-10.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNpzf3LSqPp9"
      },
      "outputs": [],
      "source": [
        "# Q3c (3 marks): Implement dataset_preprocessing tool.\n",
        "# preprocesses the dataset to work with the chosen model, and does the splits\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Global state so other tools (like training) can use the preprocessed data\n",
        "PREPROCESSED_DATA = {}\n",
        "\n",
        "def dataset_preprocessing(dataset_name: str) -> str:\n",
        "    dataset_name = dataset_name.lower().strip()\n",
        "\n",
        "    if dataset_name == \"iris\":\n",
        "        from sklearn.datasets import load_iris\n",
        "        data = load_iris(as_frame=True)\n",
        "        df = data.frame\n",
        "        X = df.drop(columns=[\"target\"])\n",
        "        y = df[\"target\"]\n",
        "\n",
        "    elif dataset_name == \"penguins\":\n",
        "        import seaborn as sns\n",
        "        df = sns.load_dataset(\"penguins\").dropna()\n",
        "        df = df.select_dtypes(include=[float, int])  # remove non-numerical columns like species\n",
        "        X = df.drop(columns=[\"body_mass_g\"]) if \"body_mass_g\" in df else df.iloc[:, :-1]\n",
        "        y = df[\"body_mass_g\"] if \"body_mass_g\" in df else df.iloc[:, -1]\n",
        "\n",
        "    else:\n",
        "        return f\"Preprocessing not supported for dataset '{dataset_name}'. Try 'iris' or 'penguins'.\"\n",
        "\n",
        "    #Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    #Split into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    #Save for other tools to use\n",
        "    PREPROCESSED_DATA[\"X_train\"] = X_train\n",
        "    PREPROCESSED_DATA[\"X_test\"] = X_test\n",
        "    PREPROCESSED_DATA[\"y_train\"] = y_train\n",
        "    PREPROCESSED_DATA[\"y_test\"] = y_test\n",
        "    PREPROCESSED_DATA[\"dataset_name\"] = dataset_name\n",
        "\n",
        "    return f\"{dataset_name.capitalize()} dataset preprocessed and split into train/test sets.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdsyTSp_qPp9"
      },
      "outputs": [],
      "source": [
        "# Q3d (3 points): Implement train_model tool.\n",
        "# trains selected model on selected dataset, the agent should not use this tool\n",
        "# on datasets and models that cannot work together.\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#Store trained model so evaluation and visualization tools can access it\n",
        "TRAINED_MODEL = {}\n",
        "\n",
        "def train_model(model_name: str) -> str:\n",
        "    if \"X_train\" not in PREPROCESSED_DATA:\n",
        "        return \"Please preprocess the dataset first using dataset_preprocessing.\"\n",
        "\n",
        "    model_name = model_name.lower().strip()\n",
        "    dataset = PREPROCESSED_DATA.get(\"dataset_name\")\n",
        "\n",
        "    X_train = PREPROCESSED_DATA[\"X_train\"]\n",
        "    y_train = PREPROCESSED_DATA[\"y_train\"]\n",
        "\n",
        "    model = None\n",
        "\n",
        "    #Only allow supported model-dataset pairs\n",
        "    if dataset in [\"iris\", \"penguins\"]:\n",
        "        if model_name == \"logistic regression\":\n",
        "            model = LogisticRegression(solver=\"lbfgs\", max_iter=200)\n",
        "        elif model_name == \"decision tree\":\n",
        "            model = DecisionTreeClassifier(max_depth=3)\n",
        "        elif model_name == \"knn\":\n",
        "            model = KNeighborsClassifier(n_neighbors=5)\n",
        "        else:\n",
        "            return f\"Model '{model_name}' not supported for tabular datasets.\"\n",
        "\n",
        "    else:\n",
        "        return f\"Training not supported for dataset '{dataset}'. Use Iris or Penguins.\"\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    TRAINED_MODEL[\"model\"] = model #Store for later use\n",
        "    TRAINED_MODEL[\"model_name\"] = model_name\n",
        "\n",
        "    return f\"{model_name.title()} model trained successfully on the {dataset} dataset.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PwGEcq3qPp9"
      },
      "outputs": [],
      "source": [
        "# Q3e (3 marks): Implement evaluate_model tool\n",
        "# evaluates the models and shows the quality metrics (accuracy, precision, and anything else of your choice)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "def evaluate_model(_: str = \"\") -> str:\n",
        "    if \"model\" not in TRAINED_MODEL or \"X_test\" not in PREPROCESSED_DATA:\n",
        "        return \"Please ensure both the model is trained and the dataset is preprocessed.\"\n",
        "\n",
        "    model = TRAINED_MODEL[\"model\"]\n",
        "    X_test = PREPROCESSED_DATA[\"X_test\"]\n",
        "    y_test = PREPROCESSED_DATA[\"y_test\"]\n",
        "\n",
        "    #Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    #Calculate key metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    #Return metrics as readable output\n",
        "    return (\n",
        "        f\"Evaluation Results:\\n\"\n",
        "        f\"Accuracy: {acc:.4f}\\n\"\n",
        "        f\"Precision: {precision:.4f}\\n\"\n",
        "        f\"Recall: {recall:.4f}\\n\"\n",
        "        f\"F1 Score: {f1:.4f}\\n\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWdsndGrqPp9"
      },
      "outputs": [],
      "source": [
        "# Q3f (5 marks): Implement visualize_results tool\n",
        "# provides results of the training/evaluation, open-ended task (2 plots minimum)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def visualize_results(_: str = \"\") -> str:\n",
        "    if \"model\" not in TRAINED_MODEL or \"X_test\" not in PREPROCESSED_DATA:\n",
        "        return \"Please ensure both the model is trained and the dataset is preprocessed.\"\n",
        "\n",
        "    model = TRAINED_MODEL[\"model\"]\n",
        "    model_name = TRAINED_MODEL[\"model_name\"]\n",
        "    X_test = PREPROCESSED_DATA[\"X_test\"]\n",
        "    y_test = PREPROCESSED_DATA[\"y_test\"]\n",
        "    dataset = PREPROCESSED_DATA[\"dataset_name\"]\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    #Plot 1: Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f\"Confusion Matrix ({model_name.title()} on {dataset.title()})\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "    #Plot 2: Feature Importance (for Decision Tree or Logistic Regression)\n",
        "    if hasattr(model, \"coef_\"):\n",
        "        importances = np.abs(model.coef_).mean(axis=0)\n",
        "    elif hasattr(model, \"feature_importances_\"):\n",
        "        importances = model.feature_importances_\n",
        "    else:\n",
        "        return \"Second visualization not supported for this model type.\"\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.barplot(x=importances, y=[f\"Feature {i}\" for i in range(len(importances))])\n",
        "    plt.title(f\"Feature Importance ({model_name.title()})\")\n",
        "    plt.xlabel(\"Importance\")\n",
        "    plt.ylabel(\"Features\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return \"Visualizations generated successfully.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyY4lATzCmsf"
      },
      "source": [
        "# Task 4: System Prompt (10 marks)\n",
        "A system prompt is essential for guiding an agent's behavior by establishing its purpose, capabilities, tone, and workflow patterns. It acts as the \"personality and instruction manual\" for the agent, defining the format of interactions (like using Thought/Action/Observation steps in our ML agent), available tools, response styles, and domain-specific knowledge—all while remaining invisible to the end user. This hidden layer of instruction ensures the agent consistently follows the intended reasoning process and operational constraints while providing appropriate and helpful responses, effectively serving as the blueprint for the agent's behavior across all interactions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCq6n5_FjJef"
      },
      "outputs": [],
      "source": [
        "# Q4a (10 marks) Build a system prompt to guide the agent based on Tutorial 9.\n",
        "# Use the following function:\n",
        "\n",
        "# Try to find alternative wording to keep the agent in the desired loop,\n",
        "# don't just copy the prompt from the tutorial.\n",
        "\n",
        "# Penalty for direct copy - 2 marks\n",
        "\n",
        "def create_agent():\n",
        "    # your system prompt goes inside the multiline string\n",
        "    system_prompt = \"\"\"\n",
        "\n",
        "    \"\"\".strip()\n",
        "\n",
        "    return ML_Agent(system_prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T16yokijI2P"
      },
      "source": [
        "# Task 5: Set the Agent Loop (10 marks)\n",
        "\n",
        "Now we are building automation of our Thought/Action/Observation sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q82GuUEmcewk"
      },
      "outputs": [],
      "source": [
        "# Q5a: (2 marks) Explain why we need the following data structure and fill it in with appropriate values:\n",
        "KNOWN_ACTIONS = {\n",
        "   # HINT See Tutorial 9.\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7A5XTqrCnCf"
      },
      "outputs": [],
      "source": [
        "# Q5b: (6 marks) Explain how the agent automation loop works line by line. Why do we need the ACTION_PATTERN variable?\n",
        "# This paper might be helpful: https://react-lm.github.io/\n",
        "\n",
        "ACTION_PATTERN = re.compile(\"^Action: (\\w+): (.*)$\")\n",
        "\n",
        "number_of_steps = 5 # adjust this number for your implementation, to avoid an infinite loop\n",
        "\n",
        "def query(question: str, max_turns: int = number_of_steps) -> List[Dict[str, str]]:\n",
        "    agent = create_agent()\n",
        "    next_prompt = question\n",
        "\n",
        "    for turn in range(max_turns):\n",
        "        result = agent(next_prompt)\n",
        "        print(result)\n",
        "        actions = [\n",
        "            ACTION_PATTERN.match(a)\n",
        "            for a in result.split(\"\\n\")\n",
        "            if ACTION_PATTERN.match(a)\n",
        "        ]\n",
        "        if actions:\n",
        "            action, action_input = actions[0].groups()\n",
        "            if action not in KNOWN_ACTIONS:\n",
        "                raise ValueError(f\"Unknown action: {action}: {action_input}\")\n",
        "            print(f\"\\n ---> Executing {action} with input: {action_input}\")\n",
        "            observation = KNOWN_ACTIONS[action](action_input)\n",
        "            print(f\"Observation: {observation}\")\n",
        "            next_prompt = f\"Observation: {observation}\"\n",
        "        else:\n",
        "            break\n",
        "    return agent.state.messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z33PNv77iwN_"
      },
      "outputs": [],
      "source": [
        "# Q5b: (2 marks)\n",
        "# QUESTION: How can we check the whole history of the agent's interaction with LLM?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8F2uGS_qPp-"
      },
      "source": [
        "# Task 6: Run your agent (15 marks)\n",
        "\n",
        "Let's see if your agent works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tfBsrMqiwLf"
      },
      "outputs": [],
      "source": [
        "# Execute any THREE example prompts using your agent. (Each working prompt exaple will give you 5 marks, 5x3=15)\n",
        "# DONT FORGET TO SAVE THE OUTPUT\n",
        "\n",
        "# User Prompt examples you should be able to give to your agent:\n",
        "# **Evaluate Linear Regression Model on Iris Dataset**\n",
        "# **Train a logistic regression model on the Iris dataset**\n",
        "# **Load the Penguins dataset and preprocess it.**\n",
        "# **Train a decision tree model on the Penguins dataset and evaluate it.**\n",
        "# **Load the CIFAR-10 dataset and train Mini-ResNet CNN, visualize results**\n",
        "\n",
        "# Use this template:\n",
        "\n",
        "# Example 1: Prompt\n",
        "print(\"\\nExample 1: Evaluate Linear Regression Model on Iris Dataset\")\n",
        "print(\"=\" * 50)\n",
        "task = \"Evaluate Linear Regression Model on Iris Dataset\"\n",
        "result = query(task)\n",
        "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIOy4EGr02D4"
      },
      "source": [
        "# Task 7: BONUS (10 points)\n",
        "Not valid without completion of all the previous tasks and tool implementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10N4ZEGjiwIv"
      },
      "outputs": [],
      "source": [
        "# Build your own additional ML-related tool and provide an example of interaction with your reasoning agent\n",
        "# using a prompt of your choice that makes the agent use your tool at one of the reasoning steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG9_BGQrG1go"
      },
      "source": [
        "Good luck!\n",
        "\n",
        "## Signature:\n",
        "Don't forget to insert your name and student number and execute the snippet below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKSDTADVqPp-"
      },
      "outputs": [],
      "source": [
        "!pip install watermark\n",
        "# Provide your Signature:\n",
        "%load_ext watermark\n",
        "%watermark -a 'Your Name, #Student_Number' -nmv --packages numpy,pandas,sklearn,matplotlib,seaborn,graphviz,groq,torch"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}